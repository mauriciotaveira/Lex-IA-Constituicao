import streamlit as st
import pandas as pd
import google.generativeai as genai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# --- 1. CONFIGURA√á√ÉO E ESTILO ---
st.set_page_config(page_title="Lex-IA 2.0 Pro", page_icon="‚öñÔ∏è", layout="wide")

# Inicializa o hist√≥rico na mem√≥ria da sess√£o
if 'historico' not in st.session_state:
    st.session_state.historico = []

st.markdown("""
    <style>
    .main { background-color: #0e1117; color: #ffffff; }
    .titulo-moderno {
        background: -webkit-linear-gradient(#00f2fe, #4facfe);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3rem;
        font-weight: 800;
    }
    .stButton>button {
        background: linear-gradient(45deg, #4facfe 0%, #00f2fe 100%);
        color: white; border: none; border-radius: 12px; font-weight: bold;
    }
    .card-historico {
        background-color: #1e2130;
        padding: 15px;
        border-radius: 10px;
        margin-bottom: 10px;
        border-left: 5px solid #4facfe;
    }
    </style>
    """, unsafe_allow_html=True)

# --- 2. CARREGAMENTO ---
@st.cache_data
def carregar_dados():
    try: return pd.read_excel("Constituicao_Mestra_V2.xlsx")
    except: return None

df = carregar_dados()

# --- 3. BARRA LATERAL (CONFIGS + HIST√ìRICO) ---
with st.sidebar:
    st.markdown("### üõ†Ô∏è Lab de IA")
    api_key = st.text_input("Sua Gemini Key", type="password")
    top_k = st.slider("Profundidade", 1, 5, 3)
    
    st.divider()
    st.markdown("### üìú Hist√≥rico de Consultas")
    if not st.session_state.historico:
        st.caption("Nenhuma consulta realizada ainda.")
    else:
        for idx, item in enumerate(reversed(st.session_state.historico)):
            with st.expander(f"üîç {item['pergunta'][:30]}..."):
                st.write(item['resposta'])
                if st.button("Limpar este", key=f"del_{idx}"):
                    st.session_state.historico.pop(len(st.session_state.historico) - 1 - idx)
                    st.rerun()

# --- 4. INTERFACE PRINCIPAL ---
st.markdown('<p class="titulo-moderno">Lex-IA 2.0 Pro</p>', unsafe_allow_html=True)

if df is not None and api_key:
    genai.configure(api_key=api_key)
    try:
        modelos = [m.name for m in genai.list_models() if "gemini" in m.name.lower()]
        modelo_escolhido = st.selectbox("Motor da IA:", modelos)
        
        st.divider()
        pergunta = st.text_input("O que deseja decifrar na Constitui√ß√£o?")

        if st.button("Analisar Agora üöÄ") and pergunta:
            # Busca RAG Turbo
            vectorizer = TfidfVectorizer(
                stop_words=['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'com', 'n√£o', 'uma', 'os', 'as', 'no', 'na', 'artigo', 'par√°grafo', 'inciso', 'constitui√ß√£o', 'regras', 'sobre'],
                max_df=0.2, ngram_range=(1, 2), sublinear_tf=True
            )
            tfidf_matrix = vectorizer.fit_transform(df['Conte√∫do'].fillna(''))
            pergunta_vec = vectorizer.transform([pergunta])
            similares = cosine_similarity(pergunta_vec, tfidf_matrix).flatten()
            indices = similares.argsort()[-10:][::-1]
            contexto = "\n".join([f"Artigo: {df.iloc[i]['Conte√∫do']}" for i in indices[:top_k]])

            with st.spinner('Elaborando parecer...'):
                model = genai.GenerativeModel(modelo_escolhido)
                prompt = f"Voc√™ √© o Lex-IA 2.0, consultor jur√≠dico s√™nior. Use tom cordial e executivo, bullet points e negrito. Contexto: {contexto}. Pergunta: {pergunta}"
                response = model.generate_content(prompt)
                
                # Salva no Hist√≥rico
                st.session_state.historico.append({"pergunta": pergunta, "resposta": response.text})
                
                # Exibe Resposta
                st.markdown("### üìù Parecer T√©cnico:")
                st.write(response.text)
                
                # --- EXIBI√á√ÉO DA RESPOSTA ---
                st.markdown("### üìù Parecer T√©cnico:")
                st.write(response.text)
                
                # --- GERADOR DA RESPOSTA (Dentro do bot√£o Analisar) ---
            with st.spinner('O Lex-IA est√° elaborando o parecer t√©cnico...'):
                model = genai.GenerativeModel(modelo_escolhido)
                prompt = f"Voc√™ √© o Lex-IA 2.0, consultor jur√≠dico s√™nior. Use tom cordial e executivo, bullet points e negrito. Contexto: {contexto}. Pergunta: {pergunta}"
                response = model.generate_content(prompt)
                
                # Salvamos tudo no estado da sess√£o (Session State)
                st.session_state.ultima_resposta = response.text
                st.session_state.indices_fontes = indices[:top_k]
                st.session_state.historico.append({"pergunta": pergunta, "resposta": response.text})

        # --- √ÅREA DE EXIBI√á√ÉO (Fora do bloco do bot√£o, para evitar repeti√ß√£o) ---
        if 'ultima_resposta' in st.session_state:
            st.divider()
            st.markdown("### üìù Parecer T√©cnico")
            
            # 1. BOT√ÉO DE C√ìPIA NO TOPO (Visibilidade Imediata)
            if st.button("üìã Copiar Parecer"):
                st.copy_to_clipboard(st.session_state.ultima_resposta)
                st.toast("Parecer copiado com sucesso!", icon="‚úÖ")
            
            # 2. O TEXTO DO PARECER
            st.markdown(st.session_state.ultima_resposta)
            
            # 3. FONTES NO FINAL (Para n√£o poluir o visual)
            st.divider()
            with st.expander("üîó Ver Fontes Originais"):
                for i in st.session_state.indices_fontes:
                    st.caption(df.iloc[i]['Conte√∫do'])
                    
    except Exception as e: st.error(f"Erro: {e}")
else:
    st.info("üëã Insira sua API Key para come√ßar.")